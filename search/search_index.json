{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"llmkit","text":"<p>Minimal, typed Python LLM wrapper. One <code>Agent</code> object, multiple providers, no boilerplate.</p> <pre><code>from llmkit import Agent, Anthropic\n\nagent = Agent(Anthropic.CLAUDE_SONNET, system=\"Be concise.\")\nreply = await agent.send(\"What is 2+2?\")\nprint(reply.text)  # \"4\"\n</code></pre>"},{"location":"#why-llmkit","title":"Why llmkit?","text":"<ul> <li>One object \u2014 <code>Agent</code> handles conversation state, tools, structured output, streaming</li> <li>Typed \u2014 Pydantic models in, validated instances out. Full type hints everywhere</li> <li>Minimal \u2014 No abstractions between you and the LLM. Read the source in 10 minutes</li> <li>Multi-provider \u2014 OpenAI, Anthropic, Gemini, Azure, Bedrock, Vertex</li> </ul>"},{"location":"#install","title":"Install","text":"<pre><code>uv add llmkit[all]         # all providers\nuv add llmkit[anthropic]   # just one\n</code></pre> <p>Requires Python 3.14+.</p>"},{"location":"getting-started/","title":"Getting Started","text":""},{"location":"getting-started/#install","title":"Install","text":"<pre><code>uv add llmkit[anthropic]\n</code></pre> <p>Or pick your provider: <code>openai</code>, <code>anthropic</code>, <code>gemini</code>, <code>azure</code>, <code>bedrock</code>, <code>vertex</code>, or <code>all</code>.</p>"},{"location":"getting-started/#basic-usage","title":"Basic usage","text":"<pre><code>import asyncio\nfrom llmkit import Agent, OpenAI\n\nasync def main():\n    agent = Agent(OpenAI.GPT_4O_MINI, system=\"Be concise.\")\n    reply = await agent.send(\"What is the capital of France?\")\n    print(reply.text)\n    print(reply.usage)  # Usage(input_tokens=..., output_tokens=...)\n\nasyncio.run(main())\n</code></pre>"},{"location":"getting-started/#multi-turn","title":"Multi-turn","text":"<p>State is managed automatically. Just keep calling <code>send</code>:</p> <pre><code>agent = Agent(\"anthropic/claude-sonnet-4-20250514\")\nawait agent.send(\"My name is Job\")\nreply = await agent.send(\"What is my name?\")\n# reply.text -&gt; \"Job\"\n</code></pre> <p>Access the full history via <code>agent.messages</code>.</p>"},{"location":"getting-started/#persisting-conversations","title":"Persisting conversations","text":"<p>Save and restore conversation state:</p> <pre><code>from llmkit.types import Message\n\n# Save\nsaved: list[Message] = agent.messages\n\n# Restore into a new agent \u2014 conversation continues\nnew_agent = Agent(\"openai/gpt-4o\", messages=saved)\nreply = await new_agent.send(\"What did we talk about?\")\n</code></pre> <p>Both <code>agent.messages</code> and the <code>messages</code> parameter are defensively copied \u2014 mutating either side won't affect the other.</p>"},{"location":"getting-started/#sync-convenience","title":"Sync convenience","text":"<p>If you don't want async:</p> <pre><code>reply = agent.send_sync(\"Quick question\")\n</code></pre>"},{"location":"getting-started/#model-selection","title":"Model selection","text":"<p>Use enums for autocomplete, or raw strings:</p> <pre><code>from llmkit import Agent, OpenAI, Anthropic, Gemini\n\nAgent(OpenAI.GPT_4O)\nAgent(Anthropic.CLAUDE_SONNET)\nAgent(Gemini.GEMINI_2_5_PRO)\n\n# Raw strings work too\nAgent(\"openai/gpt-4o\")\n</code></pre>"},{"location":"getting-started/#auth","title":"Auth","text":"<p>API keys are read from environment variables by default:</p> Provider Env var OpenAI <code>OPENAI_API_KEY</code> Anthropic <code>ANTHROPIC_API_KEY</code> Gemini <code>GOOGLE_API_KEY</code> Azure <code>AZURE_OPENAI_API_KEY</code> + <code>AZURE_OPENAI_ENDPOINT</code> <p>Override per instance:</p> <pre><code>agent = Agent(\"openai/gpt-4o\", api_key=\"sk-...\")\n</code></pre>"},{"location":"hooks/","title":"Hooks","text":"<p>Tap into the agent lifecycle to log, monitor, or react to events.</p>"},{"location":"hooks/#usage","title":"Usage","text":"<p>Register hooks with the <code>@agent.on()</code> decorator:</p> <pre><code>from llmkit import Agent, OpenAI\nfrom llmkit.types import Message, Reply\n\nagent = Agent(OpenAI.GPT_4O)\n\n@agent.on(\"turn_start\")\ndef on_turn(messages: list[Message]) -&gt; None:\n    print(f\"Sending {len(messages)} messages to the model...\")\n\n@agent.on(\"turn_end\")\ndef on_reply(reply: Reply) -&gt; None:\n    print(f\"Got reply: {reply.usage.input_tokens} in, {reply.usage.output_tokens} out\")\n</code></pre>"},{"location":"hooks/#events","title":"Events","text":"Event Signature <code>turn_start</code> <code>(messages: list[Message]) -&gt; None</code> <code>turn_end</code> <code>(reply: Reply) -&gt; None</code> <code>tool_call_start</code> <code>(name: str, args: dict[str, Any]) -&gt; None</code> <code>tool_call_end</code> <code>(name: str, args: dict[str, Any], result: str) -&gt; None</code> <p>During a tool loop, <code>turn_start</code>/<code>turn_end</code> fire on every iteration (each round-trip to the provider).</p>"},{"location":"hooks/#hook-types","title":"Hook types","text":"<p>All hook types are exported for annotation:</p> <pre><code>from llmkit import TurnStartHook, TurnEndHook, ToolCallStartHook, ToolCallEndHook\n\n# Event is a Literal type\nfrom llmkit import Event  # Literal[\"turn_start\", \"turn_end\", \"tool_call_start\", \"tool_call_end\"]\n</code></pre>"},{"location":"hooks/#async-handlers","title":"Async handlers","text":"<p>All hooks accept both sync and async handlers:</p> <pre><code>@agent.on(\"tool_call_end\")\nasync def log_to_db(name: str, args: dict[str, Any], result: str) -&gt; None:\n    await db.log(tool=name, result=result)\n</code></pre>"},{"location":"hooks/#multiple-handlers","title":"Multiple handlers","text":"<p>Register multiple handlers per event \u2014 they fire in registration order:</p> <pre><code>@agent.on(\"turn_start\")\ndef log_it(messages: list[Message]) -&gt; None:\n    logger.info(\"turn starting\")\n\n@agent.on(\"turn_start\")\ndef track_it(messages: list[Message]) -&gt; None:\n    metrics.increment(\"turns\")\n</code></pre>"},{"location":"providers/","title":"Providers","text":""},{"location":"providers/#built-in-providers","title":"Built-in providers","text":"Provider Install Model string OpenAI <code>llmkit[openai]</code> <code>openai/gpt-4o</code> Anthropic <code>llmkit[anthropic]</code> <code>anthropic/claude-sonnet-4-20250514</code> Gemini <code>llmkit[gemini]</code> <code>gemini/gemini-2.5-pro</code> Azure OpenAI <code>llmkit[azure]</code> <code>azure/gpt-4o</code> AWS Bedrock <code>llmkit[bedrock]</code> <code>bedrock/anthropic.claude-sonnet-4-20250514-v1:0</code> GCP Vertex AI <code>llmkit[vertex]</code> <code>vertex/claude-sonnet-4@20250514</code>"},{"location":"providers/#cloud-provider-setup","title":"Cloud provider setup","text":""},{"location":"providers/#azure-openai","title":"Azure OpenAI","text":"<pre><code>from llmkit import Agent\n\nagent = Agent(\n    \"azure/gpt-4o\",\n    api_key=\"...\",\n    base_url=\"https://my-resource.openai.azure.com\",\n    api_version=\"2024-10-21\",\n)\n</code></pre> <p>Or set <code>AZURE_OPENAI_API_KEY</code> and <code>AZURE_OPENAI_ENDPOINT</code> env vars.</p>"},{"location":"providers/#aws-bedrock","title":"AWS Bedrock","text":"<pre><code>from llmkit import Agent, Bedrock\n\nagent = Agent(Bedrock.CLAUDE_SONNET, aws_region=\"us-west-2\")\n</code></pre> <p>Uses your default AWS credentials (env vars, <code>~/.aws/credentials</code>, or IAM role).</p>"},{"location":"providers/#gcp-vertex-ai","title":"GCP Vertex AI","text":"<pre><code>from llmkit import Agent, Vertex\n\nagent = Agent(Vertex.CLAUDE_SONNET, project_id=\"my-project\", region=\"us-east5\")\n</code></pre> <p>Uses your default GCP credentials (<code>gcloud auth application-default login</code>).</p>"},{"location":"providers/#custom-providers","title":"Custom providers","text":"<p>Implement the <code>Provider</code> protocol and register:</p> <pre><code>from llmkit import Agent, register_provider\n\nclass OllamaProvider:\n    def __init__(self, *, model, api_key=None, **kwargs):\n        self._model = model\n\n    async def send(self, messages, **kwargs):\n        ...\n\n    async def stream(self, messages, **kwargs):\n        ...\n\nregister_provider(\"ollama\", OllamaProvider)\nagent = Agent(\"ollama/llama3\")\n</code></pre>"},{"location":"streaming/","title":"Streaming","text":"<p>Stream responses as they're generated.</p>"},{"location":"streaming/#usage","title":"Usage","text":"<pre><code>from llmkit import Agent, Anthropic\n\nagent = Agent(Anthropic.CLAUDE_SONNET)\n\nasync for chunk in agent.stream(\"Write me a short story\"):\n    print(chunk.text, end=\"\")\n</code></pre> <p>Each <code>chunk</code> is a <code>Reply</code> with:</p> <ul> <li><code>chunk.text</code> \u2014 the text fragment</li> <li><code>chunk.usage</code> \u2014 token counts (populated on final chunk for some providers)</li> <li><code>chunk.raw</code> \u2014 the raw provider chunk</li> </ul>"},{"location":"structured-output/","title":"Structured Output","text":"<p>Pass a Pydantic model to <code>response_model</code> and get a validated instance back.</p>"},{"location":"structured-output/#basic-usage","title":"Basic usage","text":"<pre><code>from pydantic import BaseModel\nfrom llmkit import Agent, Anthropic\n\nclass City(BaseModel):\n    name: str\n    country: str\n    population: int\n\nagent = Agent(Anthropic.CLAUDE_SONNET)\nreply = await agent.send(\"Tell me about Amsterdam\", response_model=City)\n\nreply.parsed  # City(name=\"Amsterdam\", country=\"Netherlands\", population=872680)\n</code></pre>"},{"location":"structured-output/#how-it-works","title":"How it works","text":"<ul> <li>Schema is extracted from your Pydantic model automatically</li> <li>Each provider's native structured output is used (OpenAI <code>json_schema</code>, Anthropic tool-use, Gemini <code>response_schema</code>)</li> <li>Response is validated with Pydantic \u2014 if validation fails, an auto-retry is attempted (configurable via <code>structured_retries</code>)</li> </ul>"},{"location":"structured-output/#nested-models","title":"Nested models","text":"<p>Anything Pydantic supports works:</p> <pre><code>class Address(BaseModel):\n    street: str\n    city: str\n    country: str\n\nclass Person(BaseModel):\n    name: str\n    age: int\n    address: Address\n    hobbies: list[str]\n\nreply = await agent.send(\"Make up a person\", response_model=Person)\n</code></pre>"},{"location":"structured-output/#access","title":"Access","text":"<ul> <li><code>reply.parsed</code> \u2014 the validated Pydantic instance</li> <li><code>reply.text</code> \u2014 raw text (may be <code>None</code> when structured output is used)</li> <li><code>reply.raw</code> \u2014 the raw provider response</li> </ul>"},{"location":"tools/","title":"Tools","text":"<p>Register functions as tools. The model calls them automatically.</p>"},{"location":"tools/#decorator","title":"Decorator","text":"<pre><code>from llmkit import Agent, OpenAI\n\nagent = Agent(OpenAI.GPT_4O)\n\n@agent.tool\ndef search(query: str) -&gt; str:\n    \"\"\"Search the web for information.\"\"\"\n    return do_search(query)\n\nreply = await agent.send(\"What's the weather in Amsterdam?\")\n# search() is called automatically, result fed back to the model\n</code></pre> <p>The function name becomes the tool name. The docstring becomes the description. Type hints become the parameter schema.</p>"},{"location":"tools/#async-tools","title":"Async tools","text":"<pre><code>@agent.tool\nasync def fetch(url: str) -&gt; str:\n    \"\"\"Fetch contents of a URL.\"\"\"\n    return await aiohttp_get(url)\n</code></pre>"},{"location":"tools/#programmatic-registration","title":"Programmatic registration","text":"<p>For tools you create at runtime:</p> <pre><code>agent.tools.register(some_function)\nagent.tools.register(fn, name=\"custom_name\", description=\"Override the docstring\")\nagent.tools.unregister(\"search\")\nagent.tools.list()  # list[ToolDef]\n</code></pre>"},{"location":"tools/#hosted-tools","title":"Hosted tools","text":"<p>Providers offer built-in tools like web search. Use them via <code>hosted_tools</code>:</p> <pre><code>from llmkit import Agent, Anthropic, WebSearch\n\nagent = Agent(Anthropic.CLAUDE_SONNET, hosted_tools=[WebSearch()])\nreply = await agent.send(\"What happened in the news today?\")\n</code></pre> <p>Works with OpenAI, Anthropic, and Gemini \u2014 llmkit translates to each provider's format.</p>"},{"location":"tools/#agent-as-tool","title":"Agent-as-tool","text":"<p>Turn an agent into a tool another agent can call:</p> <pre><code>researcher = Agent(\"anthropic/claude-sonnet-4-20250514\", system=\"You research topics thoroughly.\")\nwriter = Agent(\"openai/gpt-4o\", system=\"You write clear summaries.\")\n\nwriter.tools.register(researcher.as_tool(name=\"research\", description=\"Research a topic\"))\nreply = await writer.send(\"Write a summary about quantum computing\")\n# writer calls researcher as a tool, gets back research, writes summary\n</code></pre>"},{"location":"tools/#how-it-works","title":"How it works","text":"<p>When the model wants to call a tool:</p> <ol> <li>Model returns a tool call request</li> <li>llmkit executes the function with the provided arguments</li> <li>The result is sent back to the model</li> <li>Model either calls another tool or returns a final text response</li> </ol> <p>This loops up to <code>max_tool_iterations</code> times (default 10). If a tool raises an exception, the error message is sent to the model as the tool result.</p>"},{"location":"tools/#schema-extraction","title":"Schema extraction","text":"<p>Parameters are extracted from type hints using Pydantic's <code>TypeAdapter</code>:</p> <pre><code>@agent.tool\ndef search(query: str, max_results: int = 10, filter: str | None = None) -&gt; str:\n    \"\"\"Search for things.\"\"\"\n    ...\n</code></pre> <ul> <li><code>query</code> \u2014 required string</li> <li><code>max_results</code> \u2014 optional integer, default 10</li> <li><code>filter</code> \u2014 optional string</li> </ul>"},{"location":"types/","title":"Types","text":"<p>All data types are frozen dataclasses importable from <code>llmkit.types</code>.</p> <pre><code>from llmkit.types import Message, Reply, ToolCall, ToolDef, Usage\n</code></pre>"},{"location":"types/#reply","title":"Reply","text":"<p>Returned by <code>agent.send()</code> and <code>agent.stream()</code>.</p> <pre><code>@dataclass(frozen=True, slots=True)\nclass Reply:\n    text: str | None\n    parsed: Any           # validated Pydantic instance when using response_model\n    tool_calls: list[ToolCall]\n    usage: Usage\n    raw: Any              # raw provider response\n</code></pre>"},{"location":"types/#message","title":"Message","text":"<p>A single message in the conversation history. Access via <code>agent.messages</code>.</p> <pre><code>@dataclass(frozen=True, slots=True)\nclass Message:\n    role: str                              # \"user\", \"assistant\", or \"tool\"\n    content: str | None\n    tool_calls: list[ToolCall] | None = None\n</code></pre>"},{"location":"types/#usage","title":"Usage","text":"<p>Token counts for a request.</p> <pre><code>@dataclass(frozen=True, slots=True)\nclass Usage:\n    input_tokens: int\n    output_tokens: int\n</code></pre>"},{"location":"types/#toolcall","title":"ToolCall","text":"<p>A tool invocation requested by the model (or completed with a result).</p> <pre><code>@dataclass(frozen=True, slots=True)\nclass ToolCall:\n    id: str\n    name: str\n    args: dict[str, Any]\n    result: str | None = None\n</code></pre>"},{"location":"types/#tooldef","title":"ToolDef","text":"<p>A tool definition registered on the agent. Returned by <code>agent.tools.list()</code>.</p> <pre><code>@dataclass(frozen=True, slots=True)\nclass ToolDef:\n    name: str\n    description: str\n    parameters: dict[str, Any]   # JSON Schema\n</code></pre>"},{"location":"types/#hook-types","title":"Hook types","text":"<p>Typed callback aliases for lifecycle hooks, importable from <code>llmkit</code>.</p> <pre><code>from llmkit import Event, TurnStartHook, TurnEndHook, ToolCallStartHook, ToolCallEndHook\n\ntype Event = Literal[\"turn_start\", \"turn_end\", \"tool_call_start\", \"tool_call_end\"]\n\ntype TurnStartHook = Callable[[list[Message]], None] | Callable[[list[Message]], Awaitable[None]]\ntype TurnEndHook = Callable[[Reply], None] | Callable[[Reply], Awaitable[None]]\ntype ToolCallStartHook = Callable[[str, dict[str, Any]], None] | Callable[[str, dict[str, Any]], Awaitable[None]]\ntype ToolCallEndHook = Callable[[str, dict[str, Any], str], None] | Callable[[str, dict[str, Any], str], Awaitable[None]]\n</code></pre>"},{"location":"types/#exceptions","title":"Exceptions","text":"<pre><code>from llmkit.exceptions import LLMKitError, ProviderError, ParseError, ToolError\n</code></pre> Exception When <code>LLMKitError</code> Base for all llmkit errors <code>ProviderError</code> Error from an LLM provider <code>ParseError</code> Failed to parse structured output <code>ToolError</code> Tool loop exceeded max iterations"}]}